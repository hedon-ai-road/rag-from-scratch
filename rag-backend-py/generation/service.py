"""
Generation service for generating responses from LLMs.
"""
import json
import logging
import os
import uuid
from datetime import datetime
from typing import List, Tuple

import constants
from models.generation import (
    GenerationHistoryItem,
    GenerationModel,
    GenerationRequest,
    TokenUsage,
)

logger = logging.getLogger("rag-backend.generation")


class GenerationService:
    """
    Service for handling text generation operations.
    """

    async def generate(
        self, request: GenerationRequest
    ) -> Tuple[str, str, TokenUsage, datetime]:
        """
        Generate a response based on retrieved chunks.

        Args:
            request: Generation request with query and options

        Returns:
            Tuple of (response text, generation ID, token usage, timestamp)
        """
        query = request.query
        chunk_ids = request.chunk_ids
        model = request.model
        max_tokens = request.max_tokens
        temperature = request.temperature

        try:
            # Check if model is supported
            if model not in constants.SUPPORTED_GENERATION_MODELS:
                valid_models = ", ".join(constants.SUPPORTED_GENERATION_MODELS.keys())
                raise ValueError(
                    f"Unsupported generation model. Valid options: {valid_models}"
                )

            # Get context from chunks
            context = await self._get_chunks_context(chunk_ids)

            # In a real implementation, this would use an actual LLM API
            # For now, just return a placeholder response
            response = self._generate_placeholder_response(query, context)

            # Generate a unique ID for the generation
            gen_id = f"gen_{uuid.uuid4().hex[:8]}"

            # Create token usage
            # In a real implementation, this would be based on actual token counts
            token_usage = TokenUsage(
                prompt=len(context + query) // 4,  # Rough approximation
                completion=len(response) // 4,  # Rough approximation
                total=(len(context + query) + len(response)) // 4,
            )

            # Save the generation to history
            timestamp = datetime.utcnow()
            self._save_generation_history(
                gen_id, query, chunk_ids, response, model, timestamp
            )

            return response, gen_id, token_usage, timestamp
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            raise

    async def _get_chunks_context(self, chunk_ids: List[str]) -> str:
        """
        Get context text from chunk IDs.

        Args:
            chunk_ids: List of chunk IDs

        Returns:
            Combined context text
        """
        context_parts = []

        # Search for chunks across all files
        for chunk_dir in constants.CHUNKS_DIR.glob("*"):
            if not chunk_dir.is_dir():
                continue

            for chunk_id in chunk_ids:
                chunk_path = chunk_dir / f"{chunk_id}.json"
                if chunk_path.exists():
                    with open(chunk_path, "r") as f:
                        chunk_data = json.load(f)

                    context_parts.append(chunk_data["content"])

        if not context_parts:
            raise ValueError("No chunks found with the provided IDs")

        return "\n\n".join(context_parts)

    def _generate_placeholder_response(self, query: str, context: str) -> str:
        """
        Generate a placeholder response based on the query and context.

        Args:
            query: User query
            context: Context text

        Returns:
            Generated response
        """
        # Just a very simple placeholder that repeats the query
        return (
            f"Based on the provided context, here's an answer to your question '{query}':\n\n"
            f"This is a placeholder response from the RAG system. In a real implementation, "
            f"this would be generated by an LLM based on the {len(context.split())} words of context."
        )

    def _save_generation_history(
        self,
        gen_id: str,
        query: str,
        chunk_ids: List[str],
        response: str,
        model: str,
        timestamp: datetime,
    ) -> None:
        """
        Save a generation to history.

        Args:
            gen_id: Generation ID
            query: User query
            chunk_ids: IDs of chunks used for context
            response: Generated response
            model: Model used for generation
            timestamp: Generation timestamp
        """
        try:
            # Create history directory if it doesn't exist
            history_dir = constants.DATA_DIR / "generation_history"
            history_dir.mkdir(parents=True, exist_ok=True)

            # Create a history entry
            history_entry = {
                "gen_id": gen_id,
                "query": query,
                "used_chunks": chunk_ids,
                "response": response,
                "created_at": timestamp.isoformat(),
                "model_used": model,
            }

            # Save to file
            history_path = history_dir / f"{gen_id}.json"
            with open(history_path, "w") as f:
                json.dump(history_entry, f, indent=2)
        except Exception as e:
            logger.error(f"Error saving generation history: {str(e)}")
            # Non-critical, so just log the error

    async def get_generation_history(self, limit: int) -> List[GenerationHistoryItem]:
        """
        Get recent generation history.

        Args:
            limit: Maximum number of history items to return

        Returns:
            List of generation history items
        """
        try:
            # Get history directory
            history_dir = constants.DATA_DIR / "generation_history"
            if not history_dir.exists():
                return []

            # Get all history files
            history_files = list(history_dir.glob("*.json"))

            # Sort by modification time (newest first)
            history_files.sort(key=lambda p: os.path.getmtime(p), reverse=True)

            # Take the most recent ones
            history_files = history_files[:limit]

            # Load each history entry
            history_items = []

            for history_path in history_files:
                with open(history_path, "r") as f:
                    history_data = json.load(f)

                # Create a history item
                history_item = GenerationHistoryItem(
                    gen_id=history_data["gen_id"],
                    query=history_data["query"],
                    used_chunks=history_data["used_chunks"],
                    response=history_data["response"],
                    created_at=datetime.fromisoformat(history_data["created_at"]),
                    model_used=history_data["model_used"],
                )

                history_items.append(history_item)

            return history_items
        except Exception as e:
            logger.error(f"Error getting generation history: {str(e)}")
            raise

    async def get_supported_models(self) -> List[GenerationModel]:
        """
        Get a list of supported generation models.

        Returns:
            List of supported generation models
        """
        models = []

        # In a real implementation, this might query an API or database
        # For now, return the hardcoded models from constants
        for model_id, model_info in constants.SUPPORTED_GENERATION_MODELS.items():
            models.append(
                GenerationModel(
                    id=model_id,
                    name=model_info["name"],
                    provider=model_info["provider"],
                    max_tokens=model_info["max_tokens"],
                    description=model_info["description"],
                )
            )

        return models


if __name__ == "__main__":
    # For testing the service directly
    import asyncio

    async def test_generation_service():
        # Create test directories
        constants.DATA_DIR.mkdir(parents=True, exist_ok=True)

        service = GenerationService()

        # Test get supported models
        models = await service.get_supported_models()
        print(f"Supported models:")
        for model in models:
            print(f"  - {model.name} ({model.max_tokens} tokens) by {model.provider}")

        # Test get history
        history = await service.get_generation_history(10)
        print(f"Found {len(history)} generation history items")
        for item in history:
            print(f"  - {item.created_at}: '{item.query}'")

    asyncio.run(test_generation_service())
